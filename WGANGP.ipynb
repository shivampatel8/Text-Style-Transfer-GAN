{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open(\"neg_small.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    neg_text = []\n",
    "    for line in lines:\n",
    "        data = line.split(\"\\n\")[0]\n",
    "        neg_text.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"pos_small.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    pos_text = []\n",
    "    for line in lines:\n",
    "        data = line.split(\"\\n\")[0]\n",
    "        pos_text.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = pos_text + neg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(full_text)\n",
    "word2index = tokenizer.word_index\n",
    "VOCAB_SIZE = len(word2index) + 1\n",
    "index2word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sequences = tokenizer.texts_to_sequences(neg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sequences = tokenizer.texts_to_sequences(pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "MAX_LEN = 15\n",
    "# num_samples = len(encoder_sequences)\n",
    "# decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "neg_data = pad_sequences(neg_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "pos_data = pad_sequences(pos_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 15)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "full_data = pos_data + neg_data\n",
    "print(full_data.shape)\n",
    "print(type(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Loded!\n",
      "400001\n",
      "4.2165\n",
      "-4.1831\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "embeddings_index = {}\n",
    "max_embedding_value = -math.inf\n",
    "min_embedding_value = math.inf\n",
    "with open('glove.6B.200d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        c_max = np.amax(coefs)\n",
    "        c_min = np.amin(coefs)\n",
    "        max_embedding_value = max(max_embedding_value, c_max)\n",
    "        min_embedding_value = min(min_embedding_value, c_min)\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print(\"Glove Loded!\")\n",
    "print(len(embeddings_index))\n",
    "print(max_embedding_value)\n",
    "print(min_embedding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimention = 200\n",
    "def embedding_matrix_creater(embedding_dimention, word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embedding_matrix_creater(embedding_dimention, word_index=word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import _Merge\n",
    "from functools import partial\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "TRAINING_RATIO = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = Embedding(input_dim=VOCAB_SIZE, \n",
    "                        output_dim=200, \n",
    "                        trainable=False, \n",
    "                        input_length=MAX_LEN,\n",
    "                        weights=[embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein Loss and Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight = 10):\n",
    "     # first get the gradients:\n",
    "    #   assuming: - that y_pred has dimensions (batch_size, 1)\n",
    "    #             - averaged_samples has dimensions (batch_size, nbr_features)\n",
    "    # gradients afterwards has dimension (batch_size, nbr_features), basically\n",
    "    # a list of nbr_features-dimensional gradient vectors\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #     #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "#     gradient_l2_norm = K.sqrt(K.sum(K.square(gradients)))\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Averaged Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAveragedSamples(_Merge):\n",
    "    def _merge_function(self, inputs):\n",
    "        weights = K.random_uniform((BATCH_SIZE, 1, 1))\n",
    "        return (weights * inputs[0]) + ((1 - weights) * inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator model and Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1024, return_sequences=True))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(LSTM(1024, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(200)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1024, return_sequences=True))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator()\n",
    "discriminator = make_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 200)           7223600   \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 15, 200)           16765128  \n",
      "=================================================================\n",
      "Total params: 23,988,728\n",
      "Trainable params: 16,765,128\n",
      "Non-trainable params: 7,223,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 200)           7223600   \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 15, 200)           16765128  \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 1)                 8231169   \n",
      "=================================================================\n",
      "Total params: 32,219,897\n",
      "Trainable params: 16,765,128\n",
      "Non-trainable params: 15,454,769\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# discrminator trainable false for generator training\n",
    "\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "# for layer in generator.layers:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# generator.trainable = True\n",
    "\n",
    "input_sequences = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "embedding_output = embed_layer(input_sequences)\n",
    "\n",
    "generator_layers = generator(embedding_output)\n",
    "\n",
    "pre_train_generator_model = Model([input_sequences], [generator_layers])\n",
    "pre_train_generator_model.compile(optimizer=Adam(lr=LEARNING_RATE), \n",
    "                                  loss='mse', \n",
    "                                  metrics=['mae'])\n",
    "print(pre_train_generator_model.summary())\n",
    "\n",
    "\n",
    "discriminator_layers_for_generator = discriminator(generator_layers)\n",
    "generator_model = Model([input_sequences], [discriminator_layers_for_generator])\n",
    "generator_model.compile(optimizer=Adam(lr=0.0001), \n",
    "              loss=wasserstein_loss, \n",
    "              metrics=['mae'])\n",
    "print(generator_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yash/Downloads/yes/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 15, 200)      7223600     input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 15, 200)      16765128    embedding_1[5][0]                \n",
      "__________________________________________________________________________________________________\n",
      "random_averaged_samples_2 (Rand (None, 15, 200)      0           embedding_1[4][0]                \n",
      "                                                                 embedding_1[5][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 1)            8231169     embedding_1[4][0]                \n",
      "                                                                 sequential_3[2][0]               \n",
      "                                                                 random_averaged_samples_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 32,219,897\n",
      "Trainable params: 8,231,169\n",
      "Non-trainable params: 23,988,728\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generator trainable false for discriminator training \n",
    "for layer in generator.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "generator.trainable = False\n",
    "\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "discriminator.trainable = True\n",
    "\n",
    "# real sample input and output\n",
    "real_samples = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "real_samples_embedding = embed_layer(real_samples)\n",
    "\n",
    "discriminator_output_from_real_samples = discriminator(real_samples_embedding)\n",
    "\n",
    "\n",
    "# generator input and discriminator output for generated sentence\n",
    "generator_input_for_discriminator = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "generator_input_for_discriminator_embedding = embed_layer(generator_input_for_discriminator)\n",
    "generated_samples_for_discriminator = generator(generator_input_for_discriminator_embedding)\n",
    "\n",
    "discriminator_output_from_generator = discriminator(generated_samples_for_discriminator)\n",
    "\n",
    "\n",
    "# averaged samples output\n",
    "averaged_samples = RandomAveragedSamples()([real_samples_embedding, generator_input_for_discriminator_embedding])\n",
    "# averaged_samples_embedding = embed_layer(averaged_samples)\n",
    "discriminator_output_from_averaged_samples = discriminator(averaged_samples)\n",
    "\n",
    "# gradient penalty loss\n",
    "partial_gp_loss = partial(gradient_penalty_loss, averaged_samples=averaged_samples)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "\n",
    "discriminator_model = Model(inputs=[real_samples, \n",
    "                                    generator_input_for_discriminator],\n",
    "                            outputs=[discriminator_output_from_real_samples,\n",
    "                                     discriminator_output_from_generator, \n",
    "                                     discriminator_output_from_averaged_samples])\n",
    "\n",
    "discriminator_model.compile(optimizer=Adam(0.0001),\n",
    "                            loss=[wasserstein_loss,\n",
    "                                  wasserstein_loss,\n",
    "                                  partial_gp_loss])\n",
    "print(discriminator_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_embeddings(sentences):\n",
    "    target_data = np.zeros(shape=(len(sentences),15,200))\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, index in enumerate(sentence):\n",
    "            embedding_vector = embedding_matrix[index]\n",
    "            target_data[i,j,:] = embedding_vector\n",
    "    return target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_y = np.ones(shape=(BATCH_SIZE, 1), dtype=np.float32)\n",
    "negative_y = -positive_y\n",
    "dummy_y = np.zeros(shape=(BATCH_SIZE, 1), dtype=np.float32)\n",
    "for epoch in tqdm(range(2)):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(\"Number of batches: \", int(pos_data.shape[0] // BATCH_SIZE))\n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    pre_train_generator_loss = []\n",
    "    minibatch_size = BATCH_SIZE * TRAINING_RATIO\n",
    "    for i in range(int(pos_data.shape[0] // (minibatch_size))):\n",
    "        generator_minibatch = pos_data[i * minibatch_size: (i + 1) * minibatch_size]\n",
    "#         print(generator_minibatch.shape)\n",
    "        actual_generator_minibatch_output = get_output_embeddings(generator_minibatch)\n",
    "#         print(actual_generator_minibatch_output.shape)\n",
    "#         print(actual_generator_minibatch_output)\n",
    "        pre_train_generator_loss.append(pre_train_generator_model.train_on_batch([generator_minibatch],\n",
    "                                                                                 [actual_generator_minibatch_output]))\n",
    "        \n",
    "        discriminator_minibatch = neg_data[i * minibatch_size: (i + 1) * minibatch_size]\n",
    "        for j in range(TRAINING_RATIO):\n",
    "            neg_sentence_batch = discriminator_minibatch[j * BATCH_SIZE: (j + 1) * BATCH_SIZE]\n",
    "            d_noise = np.random.randint(0, VOCAB_SIZE + 1, size=(BATCH_SIZE, MAX_LEN))\n",
    "            discriminator_loss.append(discriminator_model.train_on_batch([neg_sentence_batch, d_noise], \n",
    "                                                                         [negative_y, positive_y, dummy_y]))\n",
    "        \n",
    "        g_noise = np.random.randint(0, VOCAB_SIZE + 1, size=(BATCH_SIZE, MAX_LEN))\n",
    "        generator_loss.append(generator_model.train_on_batch([g_noise], \n",
    "                                                             [positive_y]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for i, sentence in enumerate(test):\n",
    "    res = []\n",
    "    for j, embedding in enumerate(sentence):\n",
    "#         print(embedding)\n",
    "        res.append(nearest_word_to_embedding(embedding))\n",
    "    prediction.append(res)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def nearest_word_to_embedding(embedding):\n",
    "    # First find the word with nearest distance.\n",
    "    min_distance = math.inf\n",
    "    nearest_word = \"\"\n",
    "\n",
    "    for word, word_index in word2index.items():\n",
    "\n",
    "        distance = l2_distance(embedding, embedding_matrix[word_index])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_word = word\n",
    "\n",
    "    return nearest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
